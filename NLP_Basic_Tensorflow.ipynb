{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdfzS85VmX8E2K3IwTBAy5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import the libraries"
      ],
      "metadata": {
        "id": "ercntbgF4tZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IJFOx8mo262k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your favourite number for the random seed\n",
        "seed = 42\n",
        "\n",
        "# Sets the global random seed for numpy\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Sets the global random for tensorflow\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "print(f\"Random seed set to {seed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZuPaIHr4YHv",
        "outputId": "062d6198-40ad-4913-8f49-904a6ecfae05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data"
      ],
      "metadata": {
        "id": "tGLvhUeZ5Lwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './data/aclImdb'"
      ],
      "metadata": {
        "id": "dSo68IxO5GT5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.text_dataset_from_directory: This function creates a tf.data.Dataset object from\n",
        "# text files organized in a directory. This is useful for loading and preprocessing text data for\n",
        "# training machine learning models.\n",
        "\n",
        "# labels = 'inferred': This indicates that the labels for the dataset will be inferred from the\n",
        "# directory structure. For instance, if your directory contains subdirectories named after each\n",
        "# class, the names of these subdirectories will be used as labels.\n",
        "\n",
        "# label_mode = 'int': This specifies that the labels should be encoded as integers. If you set\n",
        "# this to 'categorical', labels would be one-hot encoded. If you set it to 'binary', it would\n",
        "# expect a binary classification.\n",
        "raw_training_set = tf.keras.utils.text_dataset_from_directory(\n",
        "    f'{data_dir}/train',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'int',\n",
        "    batch_size = 32,\n",
        "    validation_split = 0.2,\n",
        "    subset = 'training',\n",
        "    seed = seed\n",
        ")\n",
        "\n",
        "raw_validation_set = tf.keras.utils.text_dataset_from_directory(\n",
        "    f'{data_dir}/train',\n",
        "    labels = 'inferred',\n",
        "    label_model = 'int',\n",
        "    batch_size = 32,\n",
        "    validation_split = 0.2,\n",
        "    subset = 'validation',\n",
        "    seed = seed\n",
        ")\n",
        "\n",
        "# Create the test set\n",
        "raw_test_set = tf.keras.utils.text_dataset_from_directory(\n",
        "    f'{data_dir}/test',\n",
        "    labels = 'inferred',\n",
        "    label_model = 'int',\n",
        "    batch_size = 32\n",
        ")\n",
        "\n",
        "'''\n",
        "Found 5000 files belonging to 2 classes.\n",
        "Using 4000 files for training.\n",
        "Found 5000 files belonging to 2 classes.\n",
        "Using 1000 files for validation.\n",
        "Found 5000 files belonging to 2 classes.\n",
        "'''"
      ],
      "metadata": {
        "id": "700daSrP5WT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Label 0 corresponds to {raw_training_set.class_names[0]}')\n",
        "print(f'Label 1 corresponds to {raw_training_set.class_names[1]}')\n",
        "\n",
        "'''\n",
        "Label 0 corresponds to neg\n",
        "Label 1 corresponds to pos\n",
        "'''"
      ],
      "metadata": {
        "id": "SVw0kfZ164VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one batch from the daatset and print out the first three dataset in the batch\n",
        "for text_batch, label_batch in raw_training_set.take(1):\n",
        "  for i in range(3):\n",
        "    print(f\"Review:\\n {text_batch.numpy()[i]}\")\n",
        "    print(f\"Label :\\n {label_batch.numpy()[i]}\")\n",
        "\n",
        "'''\n",
        "Review:\n",
        " b'This is a reunion, a team, and a great episode of Justice. From hesitation to resolution,\n",
        " Clark has made a important leap from a troubled teenager who was afraid of a controlled destiny,\n",
        " to a Superman who, like Green Arrow, sets aside his emotions to his few loved ones, ready to save\n",
        " the whole planet. This is not just a thrilling story about teamwork, loyalty, and friendship;\n",
        " this is also about deciding what\\'s more important in life, a lesson for Clark. I do not want the\n",
        " series to end, but I hope the ensuing episodes will strictly stick to what Justice shows without any\n",
        " \"rewind\" pushes and put a good end here of Smallville---and a wonderful beginning of Superman.<br /\n",
        " ><br />In this episode, however, we should have seen more contrast between Lex and the Team. Nine\n",
        " stars should give it enough credit.'\n",
        "Label: 1\n",
        "\n",
        "Review:\n",
        "b'\"Hey Babu Riba\" is a film about a young woman, Mariana (nicknamed \"Esther\" after a famous American\n",
        "movie star), and four young men, Glenn, Sacha, Kicha, and Pop, all perhaps 15-17 years old in 1953\n",
        "Belgrade, Yugoslavia. The five are committed friends and crazy about jazz, blue jeans, or anything\n",
        "American it seems.<br /><br />The very close relationship of the teenagers is poignant, and ultimately\n",
        "a sacrifice is willingly made to try to help one of the group who has fallen on unexpected difficulties.\n",
        "In the wake of changing communist politics, they go their separate ways and reunite in 1985 (the year\n",
        "before the film was made).<br /><br />I enjoyed the film with some reservations. The subtitles for one\n",
        "thing were difficult. Especially in the beginning, there were a number of dialogues which had no subtitles\n",
        "at all. Perhaps the conversational pace required it, but I couldn\\'t always both read the text and absorb\n",
        "the scene, which caused me to not always understand which character was involved. I watched the movie (a video\n",
        "from our public library) with a friend, and neither of us really understood part of the story about acquiring\n",
        "streptomycin for a sick relative.<br /><br />This Yugoslavian coming of age film effectively conveyed the\n",
        "teenagers\\' sense of invulnerability, idealism, and strong and loyal bonds to each other. There is a main flashforward,\n",
        "and it was intriguing, keeping me guessing until the end as to who these characters were vis-a-vis the 1953 cast, and\n",
        "what had actually happened.<br /><br />I would rate it 7 out of 10, and would like to see other films by the director,\n",
        "Jovan Acin (1941-1991).'\n",
        "Label: 1\n",
        "'''"
      ],
      "metadata": {
        "id": "vegbizmI8rwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prapare the Data"
      ],
      "metadata": {
        "id": "bD6BZekR9j9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the maximum number of words\n",
        "max_features = 10000\n",
        "\n",
        "# Define the custom standardization function\n",
        "def custom_standardization(input_data):\n",
        "  # Convert all text to lowercase\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "\n",
        "  # Remove HTML tags\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "\n",
        "  # Remove punctuation\n",
        "  # tf.strings.regex_replace function to clean text data by removing punctuation.\n",
        "  # string.punctuation: This is a string containing all punctuation characters (!\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~`).\n",
        "  # re.escape(string.punctuation): This escapes all special characters in string.punctuation, so they are treated as literal characters in the regular expression.\n",
        "  # '[%s]' % ...: This formats the escaped punctuation characters into a regex character class. For example, if string.punctuation contains !\"#$%, the resulting\n",
        "  # pattern will be ['!\"#$%'], which matches any of these characters.\n",
        "  replaced = tf.strings.regex_replace(\n",
        "      stripped_html,\n",
        "      '[%s]' % re.escape(string.punctuation),\n",
        "      ''\n",
        "  )\n",
        "\n",
        "  return replaced\n",
        "\n",
        "# Create a layer thatyou can use to convert text to vectors\n",
        "# TextVectorization is a Keras preprocessing layer that converts raw text into sequences of integers or vectors. It helps in transforming text data into a format\n",
        "# suitable for input into machine learning models.\n",
        "# standardize is a parameter that specifies a custom function to standardize or preprocess text data before tokenization\n",
        "# max_tokens sets the maximum number of unique tokens (words or subwords) to keep in the vocabulary.\n",
        "# output_sequence_length sets the length of the output sequences. If the sequences of tokens are shorter than this length, they will be padded with zeros.\n",
        "# If they are longer, they will be truncated to this length. This ensures that all sequences have the same length when passed to the model, which is essential for training.\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize = custom_standardization,\n",
        "    max_tokens = max_features,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = 250\n",
        ")"
      ],
      "metadata": {
        "id": "K82Dk-M09mBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the vocabulary\n",
        "#  The function lambda x, y: x extracts only the text part (x) from each (text, label) pair in the dataset, discarding the labels (y). As a result,\n",
        "# train_text will be a dataset of text samples only.\n",
        "train_text = raw_training_set.map(lambda x,y : x)\n",
        "# .adapt(train_text): The adapt method is used to build the vocabulary from the text data. It processes the train_text dataset, extracting unique tokens\n",
        "# and their frequencies, and uses this information to create a vocabulary. The size of this vocabulary is determined by the max_tokens parameter you specified\n",
        "# earlier.\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "# Print out the vocabulary size\n",
        "print(f\"Vocabulary size : {len(vectorize_layer.get_vocabulary())}\")\n",
        "\n",
        "'''\n",
        "Vocabulary size: 10000\n",
        "'''"
      ],
      "metadata": {
        "id": "tBqlu0JiH5l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the final function that will use to vectorize the text\n",
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "# Get one batch and select the first datapoint\n",
        "text_batch, label_batch = next(iter(raw_training_set))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "\n",
        "# Show the raw data\n",
        "print(f\"Review: \\n{first_review}\")\n",
        "print(f\"\\nLabel: {raw_training_set.class_names[first_label]}\")\n",
        "# Show the vectorized data\n",
        "print(f\"\\nVectorized review\\n{vectorize_text(first_review,first_label)}\")\n",
        "\n",
        "'''\n",
        "Review:\n",
        "b\"Okay, so the plot is on shaky ground. Yeah, all right, so there are some randomly inserted song and/or dance sequences\n",
        "(for example: Adam's concert and Henri's stage act). And Leslie Caron can't really, um, you know... act.<br /><br />But\n",
        "somehow, 'An American In Paris' manages to come through it all as a polished, first-rate musical--largely on the basis of Gene\n",
        "Kelly's incredible dancing talent and choreography, and the truckloads of charm he seems to be importing into each scene with\n",
        "Caron. (He needs to, because she seems to have a... problem with emoting.) <br /><br />The most accomplished and technically\n",
        "awe-inspiring number in this musical is obviously the 16-minute ballet towards the end of the film. It's stunningly filmed,\n",
        "and Kelly and Caron dance beautifully. But my favourite number would have to be Kelly's character singing 'I Got Rhythm' with\n",
        "a bunch of French school-children, then breaking into an array of American dances. It just goes to prove how you don't need special\n",
        "effects when you've got some real *talent*.<br /><br />Not on the 'classics' level with 'Singin' In The Rain', but pretty high\n",
        "up there nonetheless. Worth the watch!\"\n",
        "\n",
        "Label: pos\n",
        "\n",
        "Vectorized review\n",
        "(<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
        "array([[ 947,   38,    2,  112,    7,   20, 6022, 1754, 1438,   31,  201,\n",
        "          38,   46,   24,   47, 6565, 8919,  603, 2928,  831,  858,   15,\n",
        "         476, 3241, 3010,    4,    1,  892,  478,    4, 3553, 5885,  175,\n",
        "          63, 6992,   21,  118,  478,   18,  813,   33,  329,    8, 1466,\n",
        "        1029,    6,  227,  143,    9,   31,   14,    3, 6590, 9055,    1,\n",
        "          20,    2, 3025,    5, 1996,    1, 1085,  914,  597,    4, 2733,\n",
        "           4,    2,    1,    5, 1411,   27,  190,    6,   26,    1,   77,\n",
        "         244,  130,   16, 5885,   27,  731,    6,   80,   53,  190,    6,\n",
        "          25,    3,  425,   16,    1,    2,   85, 3622,    4, 2603,    1,\n",
        "         593,    8,   10,  663,    7,  506,    2,    1, 4342, 1089,    2,\n",
        "         121,    5,    2,   19,   29, 5994,  886,    4, 1561,    4, 5885,\n",
        "         831, 1415,   18,   55, 1496,  593,   62,   25,    6,   26,    1,\n",
        "         105,  965,   11,  186, 4687,   16,    3,  862,    5, 1001,    1,\n",
        "          96, 2442,   77,   33, 7537,    5,  329, 4825,    9,   41,  264,\n",
        "           6, 2131,   86,   21,   87,  333,  290,  317,   51,  699,  186,\n",
        "          47,  144,  597,   23,   20,    2, 2008,  557,   16, 7714,    8,\n",
        "           2, 2477,   18,  179,  307,   57,   46, 2878,  268,    2,  106,\n",
        "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
        "  '''"
      ],
      "metadata": {
        "id": "NiNZcjlgKqh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = raw_training_set.map(vectorize_text)\n",
        "val_ds = raw_validation_set.map(vectorize_text)\n",
        "test_ds = raw_test_set.map(vectorize_text)"
      ],
      "metadata": {
        "id": "xX_EoOMxN6pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the Dataset"
      ],
      "metadata": {
        "id": "nCA4Q0KpOVHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# train_ds.prefetch(buffer_size=AUTOTUNE): This method prepares the data for the next step while the current step is being\n",
        "# executed. Setting buffer_size to AUTOTUNE enables TensorFlow to automatically determine the optimal buffer size to improve\n",
        "# the performance of your input pipeline.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size = AUTOTUNE)"
      ],
      "metadata": {
        "id": "i3MUCqjoOUmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Sequential Model"
      ],
      "metadata": {
        "id": "3rBKIewQPRpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "# Create the model by calling tf.keras.Sequential, where the layers are given in a list\n",
        "model_sequential = tf.keras.Sequential([\n",
        "    # max_features: The number of unique tokens (rows) in the embedding matrix.\n",
        "    # embedding_dim: The size of the dense vector (columns) that each token will be mapped to.\n",
        "    layers.Embedding(max_features, embedding_dim),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Print out the summary of the model\n",
        "model_sequential.summary()\n",
        "\n",
        "'''\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #\n",
        "=================================================================\n",
        " embedding (Embedding)       (None, None, 16)          160000\n",
        "\n",
        " global_average_pooling1d (  (None, 16)                0\n",
        " GlobalAveragePooling1D)\n",
        "\n",
        " dense (Dense)               (None, 1)                 17\n",
        "\n",
        "=================================================================\n",
        "Total params: 160017 (625.07 KB)\n",
        "Trainable params: 160017 (625.07 KB)\n",
        "Non-trainable params: 0 (0.00 Byte)\n",
        "'''"
      ],
      "metadata": {
        "id": "-J6WB4M8PUT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sequential.compile(\n",
        "    loss = losses.BinaryCrossentropy(),\n",
        "    optimizer = 'adam',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "pcJKVfvaP_YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Model using Functional API"
      ],
      "metadata": {
        "id": "SXe37nsZQrkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the inputs\n",
        "inputs = tf.keras.Input(shape = (None,))\n",
        "\n",
        "# Define the first layer\n",
        "embedding = layers.Embedding(max_features, embedding_dim)\n",
        "# Call the first layer with inputs as the parameter\n",
        "x = embedding(inputs)\n",
        "\n",
        "# Define the second layer\n",
        "pooling = layers.GlobalAveragePooling1D()\n",
        "# Call the second layer with x as the parameter\n",
        "x = pooling(x)\n",
        "\n",
        "# Define output layer\n",
        "outputs = layers.Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model_functional = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "# Print out the summary of the model\n",
        "model_functional.summary()\n",
        "\n",
        "'''\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #\n",
        "=================================================================\n",
        " input_1 (InputLayer)        [(None, None)]            0\n",
        "\n",
        " embedding_1 (Embedding)     (None, None, 16)          160000\n",
        "\n",
        " global_average_pooling1d_1  (None, 16)                0\n",
        "  (GlobalAveragePooling1D)\n",
        "\n",
        " dense_1 (Dense)             (None, 1)                 17\n",
        "\n",
        "=================================================================\n",
        "Total params: 160017 (625.07 KB)\n",
        "Trainable params: 160017 (625.07 KB)\n",
        "Non-trainable params: 0 (0.00 Byte)\n",
        "'''"
      ],
      "metadata": {
        "id": "P1OcdJuFQ3uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_functional.compile(\n",
        "    loss = losses.BinaryCorssentropy(),\n",
        "    optimizer = 'adam',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "FgtYR036SZNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "msv7inaRSh0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select which mode you want to use and train, the results should be the same\n",
        "model = model_functional\n",
        "\n",
        "# model = model_sequential"
      ],
      "metadata": {
        "id": "vuDL5MEbSjOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 2\n",
        ")\n",
        "\n",
        "'''\n",
        "Epoch 1/25\n",
        "125/125 - 2s - loss: 0.6904 - accuracy: 0.5490 - val_loss: 0.6869 - val_accuracy: 0.6530 - 2s/epoch - 17ms/step\n",
        "Epoch 2/25\n",
        "125/125 - 1s - loss: 0.6792 - accuracy: 0.6955 - val_loss: 0.6731 - val_accuracy: 0.7140 - 757ms/epoch - 6ms/step\n",
        "Epoch 3/25\n",
        "125/125 - 1s - loss: 0.6590 - accuracy: 0.7445 - val_loss: 0.6511 - val_accuracy: 0.7390 - 777ms/epoch - 6ms/step\n",
        "Epoch 4/25\n",
        "125/125 - 1s - loss: 0.6305 - accuracy: 0.7735 - val_loss: 0.6236 - val_accuracy: 0.7660 - 721ms/epoch - 6ms/step\n",
        "Epoch 5/25\n",
        "125/125 - 1s - loss: 0.5970 - accuracy: 0.7908 - val_loss: 0.5944 - val_accuracy: 0.7890 - 730ms/epoch - 6ms/step\n",
        "\n",
        "....\n",
        "\n",
        "Epoch 21/25\n",
        "125/125 - 1s - loss: 0.2271 - accuracy: 0.9510 - val_loss: 0.3579 - val_accuracy: 0.8620 - 701ms/epoch - 6ms/step\n",
        "Epoch 22/25\n",
        "125/125 - 1s - loss: 0.2156 - accuracy: 0.9535 - val_loss: 0.3534 - val_accuracy: 0.8610 - 691ms/epoch - 6ms/step\n",
        "Epoch 23/25\n",
        "125/125 - 1s - loss: 0.2048 - accuracy: 0.9580 - val_loss: 0.3495 - val_accuracy: 0.8610 - 699ms/epoch - 6ms/step\n",
        "Epoch 24/25\n",
        "125/125 - 1s - loss: 0.1946 - accuracy: 0.9622 - val_loss: 0.3460 - val_accuracy: 0.8610 - 717ms/epoch - 6ms/step\n",
        "Epoch 25/25\n",
        "125/125 - 1s - loss: 0.1849 - accuracy: 0.9640 - val_loss: 0.3430 - val_accuracy: 0.8610 - 835ms/epoch - 7ms/step\n",
        "'''"
      ],
      "metadata": {
        "id": "hZ5ekjGCS-BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss,accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(f\"Loss : {loss}\")\n",
        "print(f\"Accuracy : {accuracy}\")\n",
        "\n",
        "'''\n",
        "157/157 [==============================] - 2s 12ms/step - loss: 0.3644 - accuracy: 0.8452\n",
        "Loss: 0.36437687277793884\n",
        "Accuracy: 0.8452000021934509\n",
        "'''"
      ],
      "metadata": {
        "id": "3ZSqlr1wTVgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history[f'val_{metric}'])\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(metric.title())\n",
        "  plt.legend([metric, f'val_{metric}'])\n",
        "  plt.show()\n",
        "\n",
        "plot_metrics(history, \"accuracy\")\n",
        "plot_metrics(history, \"loss\")"
      ],
      "metadata": {
        "id": "vTEWVPFHTkBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "sgytpUBJVLsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new sequential model using the vctorization layer and the model just trained\n",
        "export_model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    model\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "export_model.compile(\n",
        "    # from_logits=True: The loss function will apply a sigmoid activation function to the logits before calculating the binary cross-entropy loss.\n",
        "    # from_logits=False: The loss function assumes that the predictions are already probabilities (values between 0 and 1) and calculates the binary cross-entropy loss directly from these probabilities.\n",
        "    loss = losses.BinaryCrossentropy(from_logits = False),\n",
        "    optimizer = 'adam',\n",
        "    metrics =['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "JuVrZ98VVB6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = ['this movie was very, very good', 'quite ok', 'the movie was not bad', 'bad', 'negative disappointed bad scary', 'this movie was stupid']\n",
        "\n",
        "results = export_model.predict(examples, verbose=False)\n",
        "\n",
        "for result, example in zip(results, examples):\n",
        "  print(f\"Result: {result[0]:.3f}, Label: {int(np.round(result[0]))} Review: {example}\")\n",
        "\n",
        "'''\n",
        "Result: 0.625,   Label: 1,   Review: this movie was very, very good\n",
        "Result: 0.541,   Label: 1,   Review: quite ok\n",
        "Result: 0.427,   Label: 0,   Review: the movie was not bad\n",
        "Result: 0.473,   Label: 0,   Review: bad\n",
        "Result: 0.428,   Label: 0,   Review: negative disappointed bad scary\n",
        "Result: 0.455,   Label: 0,   Review: this movie was stupid\n",
        "'''"
      ],
      "metadata": {
        "id": "MS1qT_JNVeoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
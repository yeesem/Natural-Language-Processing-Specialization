{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+VvJAZkvaNsHIpE5fljCY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oH0wFqGCWIb_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import traceback\n",
        "\n",
        "import numpy as np\n",
        "import random as  rnd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "# set random seed\n",
        "rnd.seed(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Data"
      ],
      "metadata": {
        "id": "rQKyteqaYupO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dirname = '/content/'\n",
        "filename = 'shakespeare.txt'\n",
        "lines = [] # storing all the lines in a variable.\n",
        "\n",
        "counter = 0\n",
        "\n",
        "with open(os.path.join(dirname, filename)) as files:\n",
        "    for line in files:\n",
        "        # remove leading and trailing whitespace\n",
        "        pure_line = line.strip()#.lower()\n",
        "\n",
        "        # if pure_line is not the empty string,\n",
        "        if pure_line:\n",
        "            # append it to the list\n",
        "            lines.append(pure_line)\n",
        "\n",
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqGGvL_oWYvW",
        "outputId": "136c10e4-3972-4b50-bc14-cbed5db9cd54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 6673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(lines[506:514]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8hB9LXFXxix",
        "outputId": "38c55669-6cbd-4003-b6e4-eadfe17b8223"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We'll chide this Dauphin at his father's door.\n",
            "Therefore let every man now task his thought,\n",
            "That this fair action may on foot be brought.\n",
            "Is it for fear to wet a widow's eye,\n",
            "That thou consum'st thy self in single life?\n",
            "Ah, if thou issueless shalt hap to die,\n",
            "The world will wail thee like a makeless wife,\n",
            "The world will be thy widow and still weep,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the vocabulary"
      ],
      "metadata": {
        "id": "bafIZaoRYyVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create unique character"
      ],
      "metadata": {
        "id": "MgCSamhveUIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\\n\".join(lines)\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "# Add a special character for any unknown\n",
        "vocab.insert(0, \"[UNK]\")\n",
        "# Add the empty character for padding\n",
        "vocab.insert(1, \"\")\n",
        "\n",
        "print(f\"{len(vocab)} unique characters\")\n",
        "print(\" \".join(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iN1TxTHYwqe",
        "outputId": "a7ef3b06-3c20-4c86-eca1-a1455680b9a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81 unique characters\n",
            "[UNK]  \n",
            "   ! \" ' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; < > ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert a line to tensor"
      ],
      "metadata": {
        "id": "_EoP9bGZepyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use tf.strings.unicode_split to split the text into characters.\n",
        "line = \"Hello world!\"\n",
        "chars = tf.strings.unicode_split(line, input_encoding = \"UTF-8\")\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqGddwy8ds5N",
        "outputId": "d2de8170-10f3-478f-bbaa-b150d93db6fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'H' b'e' b'l' b'l' b'o' b' ' b'w' b'o' b'r' b'l' b'd' b'!'], shape=(12,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab.index('a'))\n",
        "print(vocab.index('e'))\n",
        "print(vocab.index('i'))\n",
        "print(vocab.index('o'))\n",
        "print(vocab.index('u'))\n",
        "print(vocab.index(' '))\n",
        "print(vocab.index('2'))\n",
        "print(vocab.index('3'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOUwlR7zfAU3",
        "outputId": "a5afea1c-84a5-4b2c-c667-fc2058544c72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n",
            "59\n",
            "63\n",
            "69\n",
            "75\n",
            "3\n",
            "14\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow has a function tf.keras.layers.StringLookup that does this efficiently for list of characters.\n",
        "# Note that the output object is of type tf.Tensor. Here is the result of applying the StringLookup function\n",
        "# to the characters of \"Hello world\"\n",
        "\n",
        "# The mask_token parameter specifies a token that should be considered as a special mask token. This token is usually\n",
        "# used to indicate padding or a special state in your data.\n",
        "ids = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)(chars)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afb_MjF4fFCK",
        "outputId": "52d17a96-0cf6-44ab-995b-8534598da4ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([34 59 66 66 69  3 77 69 72 66 58  4], shape=(12,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### line_to_tensor"
      ],
      "metadata": {
        "id": "rQhBLXWCfrLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# akes in a single line and transforms each character into its unicode integer.\n",
        "# This returns a list of integers, which we'll refer to as a tensor.\n",
        "def line_to_tensors(line, vocab):\n",
        "  chars = tf.strings.unicode_split(line, input_encoding = \"UTF-8\")\n",
        "\n",
        "  ids = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)(chars)\n",
        "\n",
        "  return ids"
      ],
      "metadata": {
        "id": "Y7bcV24AftDO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function produces text given a numeric tensor"
      ],
      "metadata": {
        "id": "AUp64gjbj4ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids, vocab):\n",
        "  # Initialize the StringLookup Layer to map integer IDs back to characters\n",
        "  chars_from_ids = tf.keras.layers.StringLookup(\n",
        "      vocabulary = vocab,\n",
        "      invert = True,\n",
        "      mask_token = None\n",
        "  )\n",
        "\n",
        "  # Use the layer to decode the tensor of IDs into human-readable text\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
      ],
      "metadata": {
        "id": "Pl0chhDOhQfF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_from_ids(ids, vocab).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRVISDMGlFxK",
        "outputId": "f9865e7c-af67-4dfc-c83e-ebdb4c553e54"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello world!'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data for training and testing"
      ],
      "metadata": {
        "id": "XQ-z3ve3lcRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = lines [:-1000]\n",
        "eval_lines = lines[-1000:]\n",
        "\n",
        "print(f\"Number of training lines : {len(train_lines)}\")\n",
        "print(f\"Number of validation lines : {len(eval_lines)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBoUMBp_lemF",
        "outputId": "d73da33f-6ab3-407b-d166-2bcbbe64b160"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training lines : 5673\n",
            "Number of validation lines : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow dataset"
      ],
      "metadata": {
        "id": "LdmNzQKOl3GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = line_to_tensors(\n",
        "    \"\\n\".join([\"Hello world!\", \"Generative AI\"]),\n",
        "    vocab\n",
        ")\n",
        "\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIQ5QVGZlw28",
        "outputId": "091fe2b8-ec55-4f80-a75f-18bf4e12e5e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(26,), dtype=int64, numpy=\n",
              "array([34, 59, 66, 66, 69,  3, 77, 69, 72, 66, 58,  4,  2, 33, 59, 68, 59,\n",
              "       72, 55, 74, 63, 76, 59,  3, 27, 35])>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "print([text_from_ids([ids], vocab).numpy() for ids in ids_dataset.take(10)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUSCKv0MmX-e",
        "outputId": "ad8c5871-c07b-421b-e5cf-953aac82e4cc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'H', b'e', b'l', b'l', b'o', b' ', b'w', b'o', b'r', b'l']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# onfigure this dataset to produce batches of the same size each time\n",
        "seq_length = 10\n",
        "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder = True)"
      ],
      "metadata": {
        "id": "f9rSA6ZenVTA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in data_generator.take(2):\n",
        "  print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RC9dpMYnedq",
        "outputId": "d29cd847-ec57-48ef-9873-5c8a5ba2021a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([34 59 66 66 69  3 77 69 72 66 58], shape=(11,), dtype=int64)\n",
            "tf.Tensor([ 4  2 33 59 68 59 72 55 74 63 76], shape=(11,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for seq in data_generator.take(2):\n",
        "  print(f\"{i}. {text_from_ids(seq, vocab).numpy()}\")\n",
        "  i = i + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSOT02tan002",
        "outputId": "39eae06f-4db3-4e8b-d8da-098bf3e62ae0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. b'Hello world'\n",
            "2. b'!\\nGenerativ'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the input and the output for the model"
      ],
      "metadata": {
        "id": "hhfe5quXoLu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following function creates 2 tensors, each with a length of seq_length out of the input sequence of\n",
        "# lenght seq_length + 1. The first one contains the first seq_length elements and the second one contains\n",
        "# the last seq_length elements. For example, if you split the sequence ['H', 'e', 'l', 'l', 'o'], you will\n",
        "# obtain the sequences ['H', 'e', 'l', 'l'] and ['e', 'l', 'l', 'o'].\n",
        "def split_input_target(sequence):\n",
        "  # Create the input sequence by excluding the last char\n",
        "  input_text = sequence[:-1]\n",
        "\n",
        "  # Create the target_sequence by excluding the first char\n",
        "  target_text = sequence[1:]\n",
        "\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "_85MDecFoOM8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKId7tiTo65l",
        "outputId": "85b4f651-35e0-44bd-b96d-28ffde5ed580"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create data_generator"
      ],
      "metadata": {
        "id": "ZI0NHJkEpQvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch_dataset(lines, vocab, seq_length, batch_size = 64):\n",
        "  # Buffer size to shuffle the dataset\n",
        "  BUFFER_SIZE = 10000\n",
        "\n",
        "  # For simplicity, join all lines into a single line\n",
        "  single_line_data = \"\\n\".join(lines)\n",
        "\n",
        "  # Convert data into tensor using the given vocab\n",
        "  all_ids = line_to_tensors(single_line_data, vocab)\n",
        "\n",
        "  # Create a Tensorflow dataset from the data tensor\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "  # Create a batch dataset\n",
        "  data_generator = ids_dataset.batch(seq_length + 1, drop_remainder = True)\n",
        "\n",
        "  # Map each input sample using split_input_target function\n",
        "  dataset_xy = data_generator.map(split_input_target)\n",
        "\n",
        "  dataset = (\n",
        "      dataset_xy\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(batch_size, remainder = True)\n",
        "      .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  )\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "yaYpgFispT0T"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}
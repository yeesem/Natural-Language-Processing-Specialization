{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuOopSEuqaNPRjHrTnkqok"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oH0wFqGCWIb_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import traceback\n",
        "\n",
        "import numpy as np\n",
        "import random as  rnd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "# set random seed\n",
        "rnd.seed(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Data"
      ],
      "metadata": {
        "id": "rQKyteqaYupO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dirname = '/content/'\n",
        "filename = 'shakespeare.txt'\n",
        "lines = [] # storing all the lines in a variable.\n",
        "\n",
        "counter = 0\n",
        "\n",
        "with open(os.path.join(dirname, filename)) as files:\n",
        "    for line in files:\n",
        "        # remove leading and trai'ling whitespace\n",
        "        pure_line = line.strip()#.lower()\n",
        "\n",
        "        # if pure_line is not the empty string,\n",
        "        if pure_line:\n",
        "            # append it to the list\n",
        "            lines.append(pure_line)\n",
        "\n",
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqGGvL_oWYvW",
        "outputId": "6bbd0d17-6302-48e3-adc6-e1ea34ae0cff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 6673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(lines[506:514]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8hB9LXFXxix",
        "outputId": "2667ec9b-44e9-4c95-a61d-fa13fe8a437d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We'll chide this Dauphin at his father's door.\n",
            "Therefore let every man now task his thought,\n",
            "That this fair action may on foot be brought.\n",
            "Is it for fear to wet a widow's eye,\n",
            "That thou consum'st thy self in single life?\n",
            "Ah, if thou issueless shalt hap to die,\n",
            "The world will wail thee like a makeless wife,\n",
            "The world will be thy widow and still weep,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the vocabulary"
      ],
      "metadata": {
        "id": "bafIZaoRYyVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create unique character"
      ],
      "metadata": {
        "id": "MgCSamhveUIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\\n\".join(lines)\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "# Add a special character for any unknown\n",
        "vocab.insert(0, \"[UNK]\")\n",
        "# Add the empty character for padding\n",
        "vocab.insert(1, \"\")\n",
        "\n",
        "print(f\"{len(vocab)} unique characters\")\n",
        "print(\" \".join(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iN1TxTHYwqe",
        "outputId": "7d4d5976-30c7-43bc-c3d9-8786cc05b384"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81 unique characters\n",
            "[UNK]  \n",
            "   ! \" ' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; < > ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert a line to tensor"
      ],
      "metadata": {
        "id": "_EoP9bGZepyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use tf.strings.unicode_split to split the text into characters.\n",
        "line = \"Hello world!\"\n",
        "chars = tf.strings.unicode_split(line, input_encoding = \"UTF-8\")\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqGddwy8ds5N",
        "outputId": "5ab80f27-865b-484a-c86a-dedeb0ae0385"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'H' b'e' b'l' b'l' b'o' b' ' b'w' b'o' b'r' b'l' b'd' b'!'], shape=(12,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab.index('a'))\n",
        "print(vocab.index('e'))\n",
        "print(vocab.index('i'))\n",
        "print(vocab.index('o'))\n",
        "print(vocab.index('u'))\n",
        "print(vocab.index(' '))\n",
        "print(vocab.index('2'))\n",
        "print(vocab.index('3'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOUwlR7zfAU3",
        "outputId": "95238a3c-0741-4b8a-efcf-a8cef58ee8a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n",
            "59\n",
            "63\n",
            "69\n",
            "75\n",
            "3\n",
            "14\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow has a function tf.keras.layers.StringLookup that does this efficiently for list of characters.\n",
        "# Note that the output object is of type tf.Tensor. Here is the result of applying the StringLookup function\n",
        "# to the characters of \"Hello world\"\n",
        "\n",
        "# The mask_token parameter specifies a token that should be considered as a special mask token. This token is usually\n",
        "# used to indicate padding or a special state in your data.\n",
        "ids = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)(chars)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afb_MjF4fFCK",
        "outputId": "70c9d3be-49fa-459e-804d-32890467b10c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([34 59 66 66 69  3 77 69 72 66 58  4], shape=(12,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### line_to_tensor"
      ],
      "metadata": {
        "id": "rQhBLXWCfrLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# akes in a single line and transforms each character into its unicode integer.\n",
        "# This returns a list of integers, which we'll refer to as a tensor.\n",
        "def line_to_tensors(line, vocab):\n",
        "  chars = tf.strings.unicode_split(line, input_encoding = \"UTF-8\")\n",
        "\n",
        "  ids = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)(chars)\n",
        "\n",
        "  return ids"
      ],
      "metadata": {
        "id": "Y7bcV24AftDO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function produces text given a numeric tensor"
      ],
      "metadata": {
        "id": "AUp64gjbj4ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids, vocab):\n",
        "  # Initialize the StringLookup Layer to map integer IDs back to characters\n",
        "  chars_from_ids = tf.keras.layers.StringLookup(\n",
        "      vocabulary = vocab,\n",
        "      invert = True,\n",
        "      mask_token = None\n",
        "  )\n",
        "\n",
        "  # Use the layer to decode the tensor of IDs into human-readable text\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
      ],
      "metadata": {
        "id": "Pl0chhDOhQfF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_from_ids(ids, vocab).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRVISDMGlFxK",
        "outputId": "9621f93a-b3fb-4cc2-cd99-9265c914d41a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello world!'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data for training and testing"
      ],
      "metadata": {
        "id": "XQ-z3ve3lcRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = lines [:-1000]\n",
        "eval_lines = lines[-1000:]\n",
        "\n",
        "print(f\"Number of training lines : {len(train_lines)}\")\n",
        "print(f\"Number of validation lines : {len(eval_lines)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBoUMBp_lemF",
        "outputId": "bdba1043-e4dc-4266-acc4-86abd74d6b15"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training lines : 5673\n",
            "Number of validation lines : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow dataset"
      ],
      "metadata": {
        "id": "LdmNzQKOl3GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = line_to_tensors(\n",
        "    \"\\n\".join([\"Hello world!\", \"Generative AI\"]),\n",
        "    vocab\n",
        ")\n",
        "\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIQ5QVGZlw28",
        "outputId": "9173e739-d66a-429c-b3da-07cf124e3cfb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(26,), dtype=int64, numpy=\n",
              "array([34, 59, 66, 66, 69,  3, 77, 69, 72, 66, 58,  4,  2, 33, 59, 68, 59,\n",
              "       72, 55, 74, 63, 76, 59,  3, 27, 35])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "print([text_from_ids([ids], vocab).numpy() for ids in ids_dataset.take(10)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUSCKv0MmX-e",
        "outputId": "52d8525c-fd4a-4b9f-b322-3c024e99d7ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'H', b'e', b'l', b'l', b'o', b' ', b'w', b'o', b'r', b'l']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# onfigure this dataset to produce batches of the same size each time\n",
        "seq_length = 10\n",
        "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder = True)"
      ],
      "metadata": {
        "id": "f9rSA6ZenVTA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in data_generator.take(2):\n",
        "  print(seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RC9dpMYnedq",
        "outputId": "679c993f-7020-46de-e5f9-d7e9e526acae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([34 59 66 66 69  3 77 69 72 66 58], shape=(11,), dtype=int64)\n",
            "tf.Tensor([ 4  2 33 59 68 59 72 55 74 63 76], shape=(11,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for seq in data_generator.take(2):\n",
        "  print(f\"{i}. {text_from_ids(seq, vocab).numpy()}\")\n",
        "  i = i + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSOT02tan002",
        "outputId": "4b06c1b9-0240-4c1c-a7e7-a2f96a9c2342"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. b'Hello world'\n",
            "2. b'!\\nGenerativ'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the input and the output for the model"
      ],
      "metadata": {
        "id": "hhfe5quXoLu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following function creates 2 tensors, each with a length of seq_length out of the input sequence of\n",
        "# lenght seq_length + 1. The first one contains the first seq_length elements and the second one contains\n",
        "# the last seq_length elements. For example, if you split the sequence ['H', 'e', 'l', 'l', 'o'], you will\n",
        "# obtain the sequences ['H', 'e', 'l', 'l'] and ['e', 'l', 'l', 'o'].\n",
        "def split_input_target(sequence):\n",
        "  # Create the input sequence by excluding the last char\n",
        "  input_text = sequence[:-1]\n",
        "\n",
        "  # Create the target_sequence by excluding the first char\n",
        "  target_text = sequence[1:]\n",
        "\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "_85MDecFoOM8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list(\"tensorflow\") - ['t', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w']\n",
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKId7tiTo65l",
        "outputId": "52b63739-c758-4eb8-d627-ff090671c3cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create data_generator"
      ],
      "metadata": {
        "id": "ZI0NHJkEpQvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch_dataset(lines, vocab, seq_length, batch_size = 64):\n",
        "  # Buffer size to shuffle the dataset\n",
        "  BUFFER_SIZE = 10000\n",
        "\n",
        "  # For simplicity, join all lines into a single line\n",
        "  single_line_data = \"\\n\".join(lines)\n",
        "\n",
        "  # Convert data into tensor using the given vocab\n",
        "  all_ids = line_to_tensors(single_line_data, vocab)\n",
        "\n",
        "  # Create a Tensorflow dataset from the data tensor\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "  # Create a batch dataset\n",
        "  data_generator = ids_dataset.batch(seq_length + 1, drop_remainder = True)\n",
        "\n",
        "  # Map each input sample using split_input_target function\n",
        "  dataset_xy = data_generator.map(split_input_target)\n",
        "\n",
        "  dataset = (\n",
        "      dataset_xy\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(batch_size, drop_remainder = True)\n",
        "      .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  )\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "yaYpgFispT0T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the training dataset"
      ],
      "metadata": {
        "id": "iKn8wJGnsxB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "dataset = create_batch_dataset(train_lines, vocab, seq_length = 100, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Ir2WhquFoHRS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the GRU Language Model"
      ],
      "metadata": {
        "id": "HwyXq0fGtG2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRULM(tf.keras.Model):\n",
        "  def __init__(self, vocab_size=256, embedding_dim=256, rnn_units=128):\n",
        "      super().__init__()\n",
        "\n",
        "      # Create an embedding layer to map token indices to embedding vectors\n",
        "      self.embedding = tf.keras.layers.Embedding(\n",
        "          vocab_size, embedding_dim\n",
        "      )\n",
        "\n",
        "      # Define a GRU\n",
        "      self.gru = tf.keras.layers.GRU(\n",
        "          units = rnn_units,\n",
        "          # return_sequences=True: The GRU layer returns the output for each time step in the sequence.\n",
        "          # This is useful when you want the full sequence of outputs, such as for sequence-to-sequence\n",
        "          # tasks or when stacking multiple RNN layers.\n",
        "          return_sequences = True,\n",
        "          # return_sequences=False (default): The GRU layer returns only the output of the last time step.\n",
        "          # This is useful when you only care about the final state of the sequence, such as in tasks like\n",
        "          # sequence classification.\n",
        "          return_state = True\n",
        "      )\n",
        "\n",
        "      self.dense = tf.keras.layers.Dense(\n",
        "          units = vocab_size,\n",
        "          activation = tf.nn.log_softmax\n",
        "      )\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "      x = inputs\n",
        "      x = self.embedding(x, training = training)\n",
        "\n",
        "      if states is None:\n",
        "        # Get the initial state from the GRU layer\n",
        "        states = self.gru.get_initial_state(x)\n",
        "\n",
        "      # initial_state=states passes the previous hidden state (or the initial state if states is None) to the GRU layer.\n",
        "      # states are updated by the GRU layer and represent the internal state after processing the input sequence.\n",
        "      x, states = self.gru(x, initial_state = states, training = training)\n",
        "\n",
        "      # Predict the next tokens and apply log-softmax activation\n",
        "      x = self.dense(x, training = training)\n",
        "\n",
        "      if return_state:\n",
        "        return x, states\n",
        "      else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "gyKZOvDks84W"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup layer\n",
        "vocab_size = 82\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN Layers\n",
        "rnn_units = 512\n",
        "\n",
        "model = GRULM(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "9eIHvDWi4Gug",
        "outputId": "01a7fcfa-7b49-4f45-ffe6-00bae892c5c1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"grulm_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"grulm_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_11 (\u001b[38;5;33mGRU\u001b[0m)                         │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                         │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(\"Input : \", input_example_batch[0].numpy())\n",
        "  example_batch_predictions = model(tf.constant([input_example_batch[0].numpy()]))\n",
        "  print(\"\\n\",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZzs-Tfd7gnf",
        "outputId": "d6e2ac38-65b3-4e4a-f535-e2e3ad324173"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input :  [66 58 73 74  3 57 69 68 76 59 72 74 22  2 41 72  3 59 66 73 59  3 69 60\n",
            "  3 74 62 59 59  3 74 62 63 73  3 35  3 70 72 69 61 68 69 73 74 63 57 55\n",
            " 74 59  9  2 46 62 79  3 59 68 58  3 63 73  3 74 72 75 74 62  6 73  3 55\n",
            " 68 58  3 56 59 55 75 74 79  6 73  3 58 69 69 67  3 55 68 58  3 58 55 74\n",
            " 59 11  2 13]\n",
            "\n",
            " (1, 100, 82) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions[0][99].numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxYGJ6zxEO-w",
        "outputId": "46f4e293-3510-43f4-c9b4-058411dd81df"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.405271 , -4.3934526, -4.392767 , -4.418864 , -4.433049 ,\n",
              "       -4.3956614, -4.410755 , -4.414542 , -4.4187446, -4.400231 ,\n",
              "       -4.4112573, -4.416686 , -4.4081264, -4.407482 , -4.3992887,\n",
              "       -4.4051733, -4.387538 , -4.416606 , -4.42149  , -4.3951254,\n",
              "       -4.3954253, -4.410253 , -4.39858  , -4.4115577, -4.4029994,\n",
              "       -4.411835 , -4.3993073, -4.4315295, -4.4048543, -4.402895 ,\n",
              "       -4.4145036, -4.4188666, -4.416685 , -4.3992705, -4.403578 ,\n",
              "       -4.4018993, -4.40185  , -4.41815  , -4.4098935, -4.404907 ,\n",
              "       -4.3953876, -4.3965945, -4.4110174, -4.392286 , -4.404081 ,\n",
              "       -4.4139047, -4.4209924, -4.4046006, -4.4133706, -4.4039445,\n",
              "       -4.4066133, -4.4069643, -4.402458 , -4.4143076, -4.4103475,\n",
              "       -4.401334 , -4.392287 , -4.413392 , -4.397278 , -4.4090652,\n",
              "       -4.406566 , -4.403904 , -4.404248 , -4.4080486, -4.420021 ,\n",
              "       -4.394411 , -4.410979 , -4.41105  , -4.406652 , -4.4070473,\n",
              "       -4.3968554, -4.412951 , -4.4020777, -4.402106 , -4.3893075,\n",
              "       -4.416067 , -4.4108844, -4.4055   , -4.4241557, -4.390434 ,\n",
              "       -4.418905 , -4.3912554], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_character = tf.math.argmax(\n",
        "    example_batch_predictions[0][99]\n",
        ")\n",
        "print(last_character.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLBaOO_CETHc",
        "outputId": "745f139e-6f69-4b2d-8ec1-92b0713bb1c1"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.math.argmax(\n",
        "    example_batch_predictions[0], axis = 1\n",
        ")\n",
        "print(sampled_indices.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQdt3zL1EsAx",
        "outputId": "059cc24c-56ff-47fa-aa13-f85d188ef12d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33  9 49 26 37 39 33 15 15 74 40  5 60 79  3  3 37  3 66 66 66 66 15 39\n",
            " 39 41 67 66 66 66 41 67 33 66 37 46 39 34  3 15 33 34 15 15 41 33 33  6\n",
            " 19 66  0 62  2 50  2 37 66 34 13 70 30 76 39 19  3 69 69 63 64 37 37 37\n",
            " 59 36 39 17 81 36 81 69 62 64 76 37 13 15 15 15 39 36 36 36 39 36 36 53\n",
            " 81 74 13 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input : \\n\", text_from_ids(input_example_batch[0], vocab))\n",
        "print()\n",
        "print(\"Next char predictions : \\n\", text_from_ids(sampled_indices, vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk-lbR7iE3yN",
        "outputId": "cd83e4d4-28bb-4b30-d919-cb8edb751567"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : \n",
            " tf.Tensor(b\"ldst convert:\\nOr else of thee this I prognosticate,\\nThy end is truth's and beauty's doom and date.\\n1\", shape=(), dtype=string)\n",
            "\n",
            "Next char predictions : \n",
            " tf.Tensor(b'G,W?KMG33tN\"fy  K llll3MMOmlllOmGlKTMH 3GH33OGG\\'7l[UNK]h\\nX\\nKlH1pDvM7 ooijKKKeJM5[UNK]J[UNK]ohjvK1333MJJJMJJ[[UNK]t14', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "ZDlu6_lEFTs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "emwQRL6wFaUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model):\n",
        "  # Define the loss function\n",
        "  loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "  # Define Adam optimizer\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate = 0.00125)\n",
        "  # Compile the model\n",
        "  model.compile(optimizer = opt, loss = loss)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "l19r8oFOFNLM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "# Compile the model\n",
        "model = compile_model(model)\n",
        "# Fit the model\n",
        "history = model.fit(dataset, epochs = EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8GCRkReGz0D",
        "outputId": "75d31145-4a30-4a0e-8f87-25933c582b1b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - loss: 3.5239\n",
            "Epoch 2/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - loss: 2.4496\n",
            "Epoch 3/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - loss: 2.2230\n",
            "Epoch 4/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - loss: 2.0610\n",
            "Epoch 5/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - loss: 1.9286\n",
            "Epoch 6/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - loss: 1.8238\n",
            "Epoch 7/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 1.7313\n",
            "Epoch 8/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - loss: 1.6714\n",
            "Epoch 9/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - loss: 1.6029\n",
            "Epoch 10/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - loss: 1.5579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "k6f3tcwkHZs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### log_perplexity"
      ],
      "metadata": {
        "id": "fEb892OoJIJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_perplexity(preds, target):\n",
        "  PADDING_ID = 1\n",
        "  # Calculate the log probabilities for predictions using one-hot encoding\n",
        "  log_p = np.sum(preds * tf.one_hot(target, depth = preds.shape[-1]), axis = -1)\n",
        "\n",
        "  # Identify non-padding elements in the target\n",
        "  non_pad = 1.0 - np.equal(PADDING_ID, target)\n",
        "\n",
        "  # Apply non-padding mask to log probabilities to exclude padding\n",
        "  log_p = log_p * non_pad\n",
        "\n",
        "  # Calculate the log perplexity by taking the sum of log probabilities and dividing by the sum of non-padding\n",
        "  log_ppx = np.sum(log_p, axis = -1)/ np.sum(non_pad, axis = -1)\n",
        "\n",
        "  # Compute the mean of log perplexity\n",
        "  log_ppx = -1 * np.mean(log_ppx)\n",
        "\n",
        "  return log_ppx\n"
      ],
      "metadata": {
        "id": "EeTLE9xiHQu9"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_text = \"\\n\".join(eval_lines)\n",
        "eval_ids = line_to_tensors([eval_text], vocab)\n",
        "input_ids, target_ids = split_input_target(tf.squeeze(eval_ids, axis = 0))\n",
        "\n",
        "pred, status = model(tf.expand_dims(input_ids,0), training = False, states = None, return_state = True)\n",
        "\n",
        "# Get the log perplexity\n",
        "log_ppx = log_perplexity(pred, tf.expand_dims(target_ids, 0))\n",
        "print(f\"The log perplexity and perplexity of the model are {log_ppx} and {np.exp(log_ppx)} respectively\")"
      ],
      "metadata": {
        "id": "b-BpiWHeLKQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO01Sakd6PJPwNow6EiZwc3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlzN_J7t0WUk",
        "outputId": "33cf62f3-452a-4ad9-a816-fe89930b1f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.data.path.append('.')"
      ],
      "metadata": {
        "id": "Lh1bjHj80oWS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yeesem/NLP_Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nRZvVpU2fVY",
        "outputId": "ae20bef5-f8a5-4852-c566-b5edab4be2ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP_Dataset'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 36 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (36/36), 21.40 MiB | 4.77 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, tokenize and process the data\n",
        "import re\n",
        "with open(\"/content/NLP_Dataset/shakespeare.txt\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "data = re.sub(r'[,!?;-]', '.',data)\n",
        "data = nltk.word_tokenize(data)\n",
        "data = [ch.lower() for ch in data if ch.isalpha() or ch == '.']\n",
        "print(\"Number of tokens : \", len(data), \"\\n\", data[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtPHI51U1Tfp",
        "outputId": "30f499b8-6f97-4618-c057-aad4f536e6e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens :  60976 \n",
            " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the frequency distribution of the words in the dataset\n",
        "fdist = nltk.FreqDist(word for word in data)\n",
        "print(\"Size of vocabulary: \", len(fdist))\n",
        "print(\"Most of frequent tokens : \", fdist.most_common(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjLo_tKu2HMQ",
        "outputId": "df2db825-0ddc-4b10-ada3-97911f713eb4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  5775\n",
            "Most of frequent tokens :  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(data):\n",
        "  data = sorted(list(set(data)))\n",
        "  word2Ind = {}\n",
        "  Ind2word = {}\n",
        "  idx = 0\n",
        "  for word in data:\n",
        "    word2Ind[word] = idx\n",
        "    Ind2word[idx] = word\n",
        "    idx += 1\n",
        "  return word2Ind, Ind2word"
      ],
      "metadata": {
        "id": "rSt0NY4Q2_Yx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "print(\"Size of vocabulary: \", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOC0qSBc3TNG",
        "outputId": "c2196e26-abb0-4080-ac52-6b080f6681a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  5775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of word to index mapping\n",
        "print(\"Index of the word 'king' : \", word2Ind['king'])\n",
        "print(\"Word which has index 2743 : \", Ind2word[2743])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3vbjrds3taB",
        "outputId": "8f4d5275-d484-4852-ee9e-69d60df7ccf7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the word 'king' :  2744\n",
            "Word which has index 2743 :  kinds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "X0NGmhCN3-8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the model"
      ],
      "metadata": {
        "id": "26tpRgXA4Bgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(N, V, random_seed = 1):\n",
        "  np.random.seed(random_seed)\n",
        "\n",
        "  W1 = np.random.uniform(0,1,(N,V))\n",
        "  W2 = np.random.uniform(0,1,(V,N))\n",
        "  b1 = np.random.uniform(0,1,(N,1))\n",
        "  b2 = np.random.uniform(0,1,(V,1))\n",
        "\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "xcPjKelf4BLW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation function"
      ],
      "metadata": {
        "id": "puCFBMJa4gdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "  yhat = np.exp(z) / np.sum(np.exp(z), axis = 0)\n",
        "  return yhat"
      ],
      "metadata": {
        "id": "iXyNPYl9370m"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation"
      ],
      "metadata": {
        "id": "OlZiusFq4vAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "  h = np.dot(W1, x) + b1\n",
        "\n",
        "  # Method 1\n",
        "  #h[h < 0] = 0\n",
        "\n",
        "  # Method 2\n",
        "  h = np.maximum(h, 0)\n",
        "\n",
        "  z = np.dot(W2 ,h) + b2\n",
        "\n",
        "  return z, h"
      ],
      "metadata": {
        "id": "exCyQgMD4rU2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost function"
      ],
      "metadata": {
        "id": "xK0uufNT5YK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(y, yhat, batch_size):\n",
        "  logprobs = np.multiply(np.log(yhat), y)\n",
        "  cost = -1/batch_size * np.sum(logprobs)\n",
        "  cost = np.squeeze(cost)\n",
        "  return cost"
      ],
      "metadata": {
        "id": "iMFFyUBF5ZWR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model - Backpropagation"
      ],
      "metadata": {
        "id": "MjA6Pk3i5qWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
        "  l1 = np.dot(W2.T,yhat-y)\n",
        "\n",
        "  # Apply relu to L1\n",
        "  l1[l1 < 0] = 0\n",
        "\n",
        "  # compute the gradient for W1\n",
        "  grad_W1 = 1/batch_size * np.dot(l1,x.T)\n",
        "\n",
        "  # Compute gradient of W2\n",
        "  grad_W2 = 1/batch_size * np.dot(yhat-y,h.T)\n",
        "\n",
        "  # compute gradient for b1\n",
        "  grad_b1 = 1/batch_size * np.dot(l1,np.ones((batch_size,1)))\n",
        "\n",
        "  # compute gradient for b2\n",
        "  grad_b2 = 1/batch_size * np.dot(yhat-y,np.ones((batch_size,1)))\n",
        "  ### END CODE HERE ####\n",
        "\n",
        "  return grad_W1, grad_W2, grad_b1, grad_b2"
      ],
      "metadata": {
        "id": "EjKV-FUB5osi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent"
      ],
      "metadata": {
        "id": "BxNzigom6zwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03,\n",
        "                     random_seed=282, initialize_model=initialize_model,\n",
        "                     get_batches=get_batches, forward_prop=forward_prop,\n",
        "                     softmax=softmax, compute_cost=compute_cost,\n",
        "                     back_prop=back_prop):\n",
        "\n",
        "  W1, W2, b1, b2 = initialize_model(N, V, random_seed)\n",
        "  batch_size = 128\n",
        "  iters = 0\n",
        "  C = 2\n",
        "\n",
        "  for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "    z, h = forward_prop(x,W1,W2,b1,b2)\n",
        "\n",
        "    yhat = softmax(z)\n",
        "\n",
        "    # Compute cost\n",
        "    cost = compute_cost(y, yhat, batch_size)\n",
        "\n",
        "    if ( (iters + 1)%10 == 0):\n",
        "      print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
        "\n",
        "    # Get gradient\n",
        "    grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x,yhat,y,h,W1,W2,b1,b2,batch_size)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W1 = W1 - alpha * grad_W1\n",
        "    W2 = W2 - alpha * grad_W2\n",
        "    b1 = b1 - alpha * grad_b1\n",
        "    b2 = b2 - alpha * grad_b2\n",
        "\n",
        "    iters += 1\n",
        "    if iters == num_iters:\n",
        "      break\n",
        "    if iters % 100 == 0:\n",
        "      alpha *= 0.66\n",
        "\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "SSr3ENCQ61Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = 2\n",
        "N = 50\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "num_iters = 150\n",
        "print(\"Call gradient descent\")\n",
        "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
      ],
      "metadata": {
        "id": "0IrDlAPV74Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the word vectors"
      ],
      "metadata": {
        "id": "rtpcfmK88dQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "KylE6wxi8gWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
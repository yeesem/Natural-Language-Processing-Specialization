{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import"
      ],
      "metadata": {
        "id": "idiBRRkZkONg"
      },
      "id": "idiBRRkZkONg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22352fbc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "22352fbc",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.utils.set_random_seed(33)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f35b428",
      "metadata": {
        "id": "3f35b428"
      },
      "source": [
        "### Exploring the Data\n",
        "\n",
        "* geo: geographical entity\n",
        "* org: organization\n",
        "* per: person\n",
        "* gpe: geopolitical entity\n",
        "* tim: time indicator\n",
        "* art: artifact\n",
        "* eve: event\n",
        "* nat: natural phenomenon\n",
        "* O: filler word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75914c00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "deletable": false,
        "editable": false,
        "id": "75914c00",
        "outputId": "88584ab6-0a15-489b-db5b-eb474e129c4f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "\n",
            "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
            "\n",
            "ORIGINAL DATA:\n",
            "     Sentence #           Word  POS Tag\n",
            "0  Sentence: 1      Thousands  NNS   O\n",
            "1          NaN             of   IN   O\n",
            "2          NaN  demonstrators  NNS   O\n",
            "3          NaN           have  VBP   O\n",
            "4          NaN        marched  VBN   O\n"
          ]
        }
      ],
      "source": [
        "# display original kaggle data\n",
        "data = pd.read_csv(\"data/ner_dataset.csv\", encoding = \"ISO-8859-1\")\n",
        "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
        "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
        "print('SENTENCE:', train_sents)\n",
        "print('SENTENCE LABEL:', train_labels)\n",
        "print('ORIGINAL DATA:\\n', data.head())\n",
        "del(data, train_sents, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac8f676",
      "metadata": {
        "id": "cac8f676"
      },
      "source": [
        "### Importing the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323ddf01",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "323ddf01"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    with open(file_path,'r') as file:\n",
        "        data = np.array([line.strip() for line in file.readlines()])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4dc9e6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "1b4dc9e6"
      },
      "outputs": [],
      "source": [
        "train_sentences = load_data('data/large/train/sentences.txt')\n",
        "train_labels = load_data('data/large/train/labels.txt')\n",
        "\n",
        "val_sentences = load_data('data/large/val/sentences.txt')\n",
        "val_labels = load_data('data/large/val/labels.txt')\n",
        "\n",
        "test_sentences = load_data('data/large/test/sentences.txt')\n",
        "test_labels = load_data('data/large/test/labels.txt')\n",
        "\n",
        "'''\n",
        "train_sentences:\n",
        "array(['Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .',\n",
        "       'Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"',\n",
        "       ...\n",
        "\n",
        "train_labels:\n",
        "array(['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O',\n",
        "       'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O',\n",
        "       'O O O O O O O O O O O B-geo I-geo O', ...,\n",
        "       'B-per I-per O O O B-tim O O O O O O O O O O',\n",
        "       'B-gpe O B-per I-per O O O O O B-org I-org I-org O O O O',\n",
        "       'O O O O O O B-geo O O O O O O O O O O O O O O O O'], dtype='<U287')\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911d817d",
      "metadata": {
        "id": "911d817d"
      },
      "source": [
        "### Encoding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdbd7d0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8bdbd7d0"
      },
      "source": [
        "### Encoding the sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05ec60b8",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "05ec60b8"
      },
      "outputs": [],
      "source": [
        "def get_sentence_vectorizer(sentences):\n",
        "    tf.keras.utils.set_random_seed(33)\n",
        "\n",
        "    # Define TextVectorization object with the appropriate standardize parameter\n",
        "    # tf.keras.layers.TextVectorization to transform the sentences into integers\n",
        "\n",
        "    # By default, standardize = 'lower_and_strip_punctuation', this means the parser\n",
        "    # will remove all punctuation and make everything lowercase\n",
        "\n",
        "    # Note that this may influence the NER task, since an upper case in the middle\n",
        "    # of a sentence may indicate an entity\n",
        "    # Thus in this case, set standardize = None\n",
        "\n",
        "    # `tf.keras.layers.TextVectorization` will also pad the sentences. In this case,\n",
        "    # it will always pad using the largest sentence in the set you call it with.\n",
        "    sentence_vectorizer = tf.keras.layers.TextVectorization(standardize = None)\n",
        "    # Adapt the sentence vectorization object to the given sentences\n",
        "    sentence_vectorizer.adapt(sentences)\n",
        "    # Get the vocabulary\n",
        "    vocab = sentence_vectorizer.get_vocabulary()\n",
        "\n",
        "    return sentence_vectorizer, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05ec699",
      "metadata": {
        "deletable": false,
        "id": "05ec699",
        "outputId": "ab4c58fb-0bbf-439f-eccb-85b8d8520b7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test vocab size: 4650\n",
            "Sentence: I like learning new NLP models !\n",
            "Sentence vectorized: [ 296  314    1   59    1    1 4649]\n"
          ]
        }
      ],
      "source": [
        "test_vectorizer, test_vocab = get_sentence_vectorizer(train_sentences[:1000])\n",
        "print(f\"Test vocab size: {len(test_vocab)}\")\n",
        "\n",
        "sentence = \"I like learning new NLP models !\"\n",
        "sentence_vectorized = test_vectorizer(sentence)\n",
        "print(f\"Sentence: {sentence}\\nSentence vectorized: {sentence_vectorized}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c869d8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "a7c869d8"
      },
      "outputs": [],
      "source": [
        "sentence_vectorizer, vocab = get_sentence_vectorizer(train_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49512450",
      "metadata": {
        "id": "49512450"
      },
      "source": [
        "### Encoding the labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727b049f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "727b049f",
        "outputId": "8482e147-070f-4239-ba95-9a98af1cfddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
            "Labels: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n"
          ]
        }
      ],
      "source": [
        "print(f\"Sentence: {train_sentences[0]}\")\n",
        "print(f\"Labels: {train_labels[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6123a983",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "6123a983"
      },
      "outputs": [],
      "source": [
        "def get_tags(labels):\n",
        "    tag_set = set()\n",
        "    for el in labels:\n",
        "        for tag in el.split(\" \"):\n",
        "            tag_set.add(tag)\n",
        "    tag_list = list(tag_set)\n",
        "    tag_list.sort()\n",
        "    return tag_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb3d4be",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "3cb3d4be",
        "outputId": "aefb9efa-cf1b-4b67-8d60-ffbe0e855403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n"
          ]
        }
      ],
      "source": [
        "tags = get_tags(train_labels)\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88043496",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "88043496"
      },
      "outputs": [],
      "source": [
        "def make_tag_map(tags):\n",
        "    tag_map = {}\n",
        "    for i,tag in enumerate(tags):\n",
        "        tag_map[tag] = i\n",
        "    return tag_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bcfdscb4",
      "metadata": {
        "id": "1bcfdscb4",
        "outputId": "4ee41613-0eae-408e-bdec-9f6b401a2b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n"
          ]
        }
      ],
      "source": [
        "tag_map = make_tag_map(tags)\n",
        "print(tag_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd640d3",
      "metadata": {
        "id": "7bd640d3"
      },
      "source": [
        "### Padding the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28c6804",
      "metadata": {
        "id": "a28c6804"
      },
      "source": [
        "### Building the label vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a0a6532",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "0a0a6532"
      },
      "outputs": [],
      "source": [
        "def label_vectorizer(labels, tag_map):\n",
        "\n",
        "    label_ids = []\n",
        "\n",
        "    # Each element in labels is a string of tags so for each of them:\n",
        "    for element in labels:\n",
        "\n",
        "        tokens = element.split(\" \")\n",
        "\n",
        "        # Use the dictionaty tag_map passed as an argument to the label_vectorizer function\n",
        "        # to make the correspondence between tags and numbers.\n",
        "        element_ids = []\n",
        "\n",
        "        for token in tokens:\n",
        "            element_ids.append(tag_map[token])\n",
        "\n",
        "        # Append the found ids to corresponding to the current element to label_ids list\n",
        "        label_ids.append(element_ids)\n",
        "\n",
        "    # Pad the elements\n",
        "    # utility function in TensorFlow that helps to standardize the lengths of sequences in a dataset\n",
        "\n",
        "    label_ids = tf.keras.utils.pad_sequences(label_ids, padding = 'post', value = -1)\n",
        "\n",
        "    return label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02l23421",
      "metadata": {
        "deletable": false,
        "id": "02l23421",
        "outputId": "9de2a403-2745-4224-f4fc-a860fc554c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: The party is divided over Britain 's participation in the Iraq conflict and the continued deployment of 8,500 British troops in that country .\n",
            "Labels: O O O O O B-gpe O O O O B-geo O O O O O O O B-gpe O O O O O\n",
            "Vectorized labels: [[16 16 16 16 16  3 16 16 16 16  2 16 16 16 16 16 16 16  3 16 16 16 16 16]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Sentence: {train_sentences[5]}\")\n",
        "print(f\"Labels: {train_labels[5]}\")\n",
        "print(f\"Vectorized labels: {label_vectorizer([train_labels[5]], tag_map)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82abf2c8",
      "metadata": {
        "id": "82abf2c8"
      },
      "source": [
        "### Building the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "303db421",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "303db421"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(sentences, labels, sentence_vectorizer, tag_map):\n",
        "    sentences_ids = sentence_vectorizer(sentences)\n",
        "    labels_ids = label_vectorizer(labels, tag_map = tag_map)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((sentences_ids, labels_ids))\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc31ffcc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "bc31ffcc"
      },
      "outputs": [],
      "source": [
        "train_dataset = generate_dataset(train_sentences,train_labels, sentence_vectorizer, tag_map)\n",
        "val_dataset = generate_dataset(val_sentences,val_labels,  sentence_vectorizer, tag_map)\n",
        "test_dataset = generate_dataset(test_sentences, test_labels,  sentence_vectorizer, tag_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a0c9e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "deletable": false,
        "editable": false,
        "id": "a2a0c9e1",
        "outputId": "db098ed6-4351-41f7-cfdb-e45dd3798ebf",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of outputs is 17\n",
            "Num of vocabulary words in the training set: 29847\n",
            "The training size is 33570\n",
            "The validation size is 7194\n",
            "An example of the first sentence is\n",
            "\t [1046    6 1121   18 1832  232  543    7  528    2  158    5   60    9\n",
            "  648    2  922    6  192   87   22   16   54    3    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "An example of its corresponding label is\n",
            "\t [16 16 16 16 16 16  2 16 16 16 16 16  2 16 16 16 16 16  3 16 16 16 16 16\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ],
      "source": [
        "# Exploring information about the training data\n",
        "print(f'The number of outputs is {len(tags)}')\n",
        "# The number of vocabulary tokens (including <PAD>)\n",
        "g_vocab_size = len(vocab)\n",
        "print(f\"Num of vocabulary words in the training set: {g_vocab_size}\")\n",
        "print('The training size is', len(train_dataset))\n",
        "print('The validation size is', len(val_dataset))\n",
        "print('An example of the first sentence is\\n\\t', next(iter(train_dataset))[0].numpy())\n",
        "print('An example of its corresponding label is\\n\\t', next(iter(train_dataset))[1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3ad50c",
      "metadata": {
        "id": "2f3ad50c"
      },
      "source": [
        "### Considerations about RNNs and LSTMs inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f9e7dcc",
      "metadata": {
        "id": "7f9e7dcc"
      },
      "source": [
        "### Building the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2493e30",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "b2493e30"
      },
      "outputs": [],
      "source": [
        "# Create a Named Entity Recognition (NER) model.\n",
        "def NER(len_tags, vocab_size, embedding_dim = 50):\n",
        "\n",
        "    model = tf.keras.Sequential(name = 'sequential')\n",
        "    # Add the tf.keras.layers.Embedding layer and  mask out the zeros\n",
        "    # input_dim typically needs to be increased by 1. This adjustment accounts for the special handling of the zero value in the input sequences,\n",
        "    # which is used for padding.\n",
        "    model.add(tf.keras.layers.Embedding(input_dim = vocab_size + 1,output_dim = embedding_dim ,mask_zero = True))\n",
        "    # Add the LSTM layer. Make sure you are passing the right dimension (defined in the docstring above)\n",
        "    # and returning every output for the tf.keras.layers.LSTM layer and not the very last one.\n",
        "    model.add(tf.keras.layers.LSTM(units = embedding_dim, return_sequences = True))\n",
        "    # Add the final tf.keras.layers.Dense with the appropriate activation function. Remember you must pass the activation function itself ant not its call!\n",
        "    # You must use tf.nn.log_softmax instead of tf.nn.log_softmax().\n",
        "    model.add(tf.keras.layers.Dense(len_tags ,activation = tf.nn.log_softmax))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6ab3d3",
      "metadata": {
        "id": "4f6ab3d3"
      },
      "source": [
        "### Masked loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f02eca7a",
      "metadata": {
        "id": "f02eca7a"
      },
      "source": [
        "\n",
        "1. `from_logits`: This indicates if the values are raw values or normalized values (probabilities). Since the last layer of the model finishes with a LogSoftMax call, the results are **not** normalized - they do not lie between 0 and 1.\n",
        "2. `ignore_class`: This indicates which class should be ignored when computing the crossentropy. Remember that the class related to padding value is set to be 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1acb3d2",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "c1acb3d2"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, ignore_class = -1)\n",
        "\n",
        "    loss = loss_fn(y_true,y_pred)\n",
        "\n",
        "    return  loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb18b1a1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "eb18b1a1",
        "outputId": "aa27cc7a-7860-4348-fdab-e14d75406c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.1242604, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "true_labels = [0,1,2,0]\n",
        "predicted_logits = [[-2.3,-0.51,-1.20] , [-1.61,-0.36,-2.30], [-2.30, -0.69,-0.92], [-0.92,-0.92,-1.61]]\n",
        "print(masked_loss(true_labels, predicted_logits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50fc477",
      "metadata": {
        "deletable": false,
        "tags": [
          "graded"
        ],
        "id": "f50fc477"
      },
      "outputs": [],
      "source": [
        "def masked_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate masked accuracy for predicted labels.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (tensor): True labels.\n",
        "    y_pred (tensor): Predicted logits.\n",
        "\n",
        "    Returns:\n",
        "    accuracy (tensor): Masked accuracy.\n",
        "\n",
        "    \"\"\"\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    # You must always cast the tensors to the same type in order to use them in training. Since you will make divisions, it is safe to use tf.float32 data type.\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    # Create the mask, i.e., the values that will be ignored\n",
        "    mask = tf.math.not_equal(y_true, -1)\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "    # Perform argmax to get the predicted values\n",
        "    y_pred_class = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred_class = tf.cast(y_pred_class, tf.float32)\n",
        "    # Compare the true values with the predicted ones\n",
        "    matches_true_pred  = tf.equal(y_true, y_pred_class)\n",
        "    matches_true_pred = tf.cast(matches_true_pred , tf.float32)\n",
        "    # Multiply the acc tensor with the masks\n",
        "    matches_true_pred *= mask\n",
        "    # Compute masked accuracy (quotient between the total matches and the total valid values, i.e., the amount of non-masked values)\n",
        "    masked_acc = tf.reduce_sum(matches_true_pred)/tf.reduce_sum(mask)\n",
        "\n",
        "    return masked_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27aa010",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "f27aa010",
        "outputId": "136a98f6-d55a-4208-8790-99b237145996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.5, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "true_labels = [0,1,2,0]\n",
        "predicted_logits = [[0.1,0.6,0.3] , [0.2,0.7,0.1], [0.1, 0.5,0.4], [0.4,0.4,0.2]]\n",
        "print(masked_accuracy(true_labels, predicted_logits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "979d44f5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "979d44f5",
        "outputId": "4704fefd-4479-4065-e26e-8fc14c3bfc3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, None, 50)          1492400   \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, None, 50)          20200     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, None, 17)          867       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1513467 (5.77 MB)\n",
            "Trainable params: 1513467 (5.77 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = NER(len(tag_map), len(vocab))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca89d52a",
      "metadata": {
        "id": "ca89d52a"
      },
      "source": [
        "### Note on padding\n",
        "Padding does not affect the model's output. Of course the output dimension will change. If ten zeros are added at the end of the tensor, then the resulting output dimension will have 10 more elements (more specifically, 10 more arrays of length 17 each). However, those are removed from any calculation further on, so it won't impact at all the model's performance and training. You will be using the function tf.expand_dims."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c943c7b0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "c943c7b0"
      },
      "outputs": [],
      "source": [
        "x = tf.expand_dims(np.array([545, 467, 896]), axis = 0) # Expanding dims is needed to pass it to the model,\n",
        "                                                        # since it expects batches and not single prediction arrays\n",
        "\n",
        "x_padded = tf.expand_dims(np.array([545, 467, 896, 0, 0, 0]), axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8175d1c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "b8175d1c",
        "outputId": "8147e08b-42d8-4351-90ea-19a7bb5920f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x shape: (1, 3, 17)\n",
            "x_padded shape: (1, 6, 17)\n"
          ]
        }
      ],
      "source": [
        "pred_x = model(x)\n",
        "pred_x_padded = model(x_padded)\n",
        "print(f'x shape: {pred_x.shape}\\nx_padded shape: {pred_x_padded.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd0ad65",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "2bd0ad65",
        "outputId": "374bb074-2b9c-49b2-ac47-257287f79cd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.allclose(pred_x, pred_x[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c97826b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "3c97826b",
        "outputId": "92ea9a2b-be6e-486f-f45e-e3ebafedcef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "masked_loss is the same: True\n",
            "masked_accuracy is the same: True\n"
          ]
        }
      ],
      "source": [
        "y_true = tf.expand_dims([16, 6, 12], axis = 0)\n",
        "y_true_padded = tf.expand_dims([16,6,12,-1,-1,-1], axis = 0) # Remember you mapped the padded values to -1 in the labels\n",
        "print(f\"masked_loss is the same: {np.allclose(masked_loss(y_true,pred_x), masked_loss(y_true_padded,pred_x_padded))}\")\n",
        "print(f\"masked_accuracy is the same: {np.allclose(masked_accuracy(y_true,pred_x), masked_accuracy(y_true_padded,pred_x_padded))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba387fb9",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "ba387fb9"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "              loss = masked_loss,\n",
        "               metrics = [masked_accuracy])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71405c63",
      "metadata": {
        "id": "71405c63"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b1b8be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "deletable": false,
        "editable": false,
        "id": "84b1b8be",
        "outputId": "fbbbda7d-b6dd-42e4-a4c6-58c0a6e349b6",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "525/525 [==============================] - 41s 71ms/step - loss: 0.2637 - masked_accuracy: 0.9321 - val_loss: 0.1391 - val_masked_accuracy: 0.9581\n",
            "Epoch 2/2\n",
            "525/525 [==============================] - 4s 8ms/step - loss: 0.1090 - masked_accuracy: 0.9661 - val_loss: 0.1331 - val_masked_accuracy: 0.9590\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fe015d64ca0>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.keras.utils.set_random_seed(33)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model.fit(train_dataset.batch(BATCH_SIZE),\n",
        "          validation_data = val_dataset.batch(BATCH_SIZE),\n",
        "          shuffle=True,\n",
        "          epochs = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2270ad1f",
      "metadata": {
        "id": "2270ad1f"
      },
      "source": [
        "### Compute Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78687bf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "deletable": false,
        "editable": false,
        "id": "78687bf7",
        "outputId": "701d6b4d-b9b6-41f7-80b3-d0c043880704",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - 1s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Convert the sentences into ids\n",
        "test_sentences_id = sentence_vectorizer(test_sentences)\n",
        "# Convert the labels into token ids\n",
        "test_labels_id = label_vectorizer(test_labels,tag_map)\n",
        "# Rename to prettify next function call\n",
        "y_true = test_labels_id\n",
        "y_pred = model.predict(test_sentences_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff0e106",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [],
        "id": "eff0e106",
        "outputId": "b6d5881a-775d-4d5e-ae9e-6daf90f9757f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model's accuracy in test set is: 0.9588\n"
          ]
        }
      ],
      "source": [
        "print(f\"The model's accuracy in test set is: {masked_accuracy(y_true,y_pred).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d578a163",
      "metadata": {
        "id": "d578a163"
      },
      "source": [
        "### Testing with your Own Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b7025a",
      "metadata": {
        "deletable": false,
        "id": "14b7025a",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def predict(sentence, model, sentence_vectorizer, tag_map):\n",
        "    \"\"\"\n",
        "    Predict NER labels for a given sentence using a trained model.\n",
        "\n",
        "    Parameters:\n",
        "    sentence (str): Input sentence.\n",
        "    model (tf.keras.Model): Trained NER model.\n",
        "    sentence_vectorizer (tf.keras.layers.TextVectorization): Sentence vectorization layer.\n",
        "    tag_map (dict): Dictionary mapping tag IDs to labels.\n",
        "\n",
        "    Returns:\n",
        "    predictions (list): Predicted NER labels for the sentence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the sentence into ids\n",
        "    sentence_vectorized = sentence_vectorizer(sentence)\n",
        "    # Expand its dimension to make it appropriate to pass to the model\n",
        "    sentence_vectorized = tf.expand_dims(sentence_vectorized, axis = 0)\n",
        "    # Get the model output\n",
        "    output = model(sentence_vectorized)\n",
        "    # Get the predicted labels for each token, using argmax function and specifying the correct axis to perform the argmax\n",
        "    outputs = np.argmax(output, axis = -1)\n",
        "    # Next line is just to adjust outputs dimension. Since this function expects only one input to get a prediction, outputs will be something like [[1,2,3]]\n",
        "    # so to avoid heavy notation below, let's transform it into [1,2,3]\n",
        "    outputs = outputs[0]\n",
        "    # Get a list of all keys, remember that the tag_map was built in a way that each label id matches its index in a list\n",
        "    labels = list(tag_map.keys())\n",
        "    pred = []\n",
        "    # Iterating over every predicted token in outputs list\n",
        "    for tag_idx in outputs:\n",
        "        pred_label = labels[tag_idx]\n",
        "        pred.append(pred_label)\n",
        "\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be0189a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "deletable": false,
        "id": "1be0189a",
        "outputId": "fab815fd-0472-4eaf-968a-abff1f5cfff5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 6 14 16 16  5 13 16 16 16 16 16 16 16  5 16 16 16 16 16 16  7 15 16 16\n",
            "  5 13 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
            " 16 16 16 16]\n",
            "Peter B-per\n",
            "Parker I-per\n",
            "White B-org\n",
            "House I-org\n",
            "U.S B-org\n",
            "Sunday B-tim\n",
            "morning I-tim\n",
            "White B-org\n",
            "House I-org\n"
          ]
        }
      ],
      "source": [
        "# Try the output for the introduction example\n",
        "#sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
        "#sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
        "\n",
        "# New york times news:\n",
        "sentence = \"Peter Parker , the White House director of trade and manufacturing policy of U.S , said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall , though he said it wouldn â€™t necessarily come\"\n",
        "predictions = predict(sentence, model, sentence_vectorizer, tag_map)\n",
        "for x,y in zip(sentence.split(' '), predictions):\n",
        "    if y != 'O':\n",
        "        print(x,y)"
      ]
    }
  ],
  "metadata": {
    "grader_version": "1",
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
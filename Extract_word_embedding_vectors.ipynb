{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZljNbCmKbJ7ZUJqnCoFPG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj_I0sL9RcS4"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(words):\n",
        "  words = sorted(list(set(words)))\n",
        "  n = len(words)\n",
        "  word2Ind = {}\n",
        "  Ind2word = {}\n",
        "  idx = 0\n",
        "  for word in words:\n",
        "    word2Ind[word] = idx\n",
        "    Ind2word[idx] = word\n",
        "    idx += 1\n",
        "  return word2Ind, Ind2word"
      ],
      "metadata": {
        "id": "ufgJaSDTSFAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Define V. Remember this is the size of the vocabulary\n",
        "V = 5\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "\n",
        "# Define first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weights\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ],
      "metadata": {
        "id": "58ghMTssSI5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting word embedding vectors"
      ],
      "metadata": {
        "id": "nM0iERNTS55c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 1 : extract embedding vectors from W1"
      ],
      "metadata": {
        "id": "S1rSKLxQS863"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print W1\n",
        "W1"
      ],
      "metadata": {
        "id": "PTsZrImgSgX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print corresponding word for each index within vocabulary's range\n",
        "for i in range(V):\n",
        "  print(Ind2word[i])"
      ],
      "metadata": {
        "id": "uRc7K6bqTdNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "  # Extract the column corresponding to the index of the word in the vocabulary\n",
        "  word_embedding_vector = W1[:, word2Ind[word]]\n",
        "  # Print word alongside word embedding vector\n",
        "  print(f\"{word}: {word_embedding_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMiaJsZLTlgp",
        "outputId": "b8af4610-3dc2-41d2-af1d-84b1d68bee8b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [0.41687358 0.32735501 0.26637602]\n",
            "because: [ 0.08854191  0.22795148 -0.23846886]\n",
            "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
            "i: [ 0.28320538  0.4117634  -0.11399446]\n",
            "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 2: extract embedding vectors from W2"
      ],
      "metadata": {
        "id": "VgezeU__V_aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print transpose W2\n",
        "W2.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VU0jAVxWC4h",
        "outputId": "9686b380-3be4-451f-ea3c-010f9ae5ee0f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n",
              "       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n",
              "       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "  # Extract the column corresponding to the index of the word index in the vocabulary\n",
        "  word_embedding_vector = W2.T[:, word2Ind[word]]\n",
        "  # Print word alongside word embedding vector\n",
        "  print(f\"{word} : {word_embedding_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDPlxRhJWGqt",
        "outputId": "a52ceebe-4a67-42b3-e81a-9ef7550a040f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am : [-0.22182064 -0.43008631  0.13310965]\n",
            "because : [0.08476603 0.08123194 0.1772054 ]\n",
            "happy : [ 0.1871551  -0.06107263 -0.1790735 ]\n",
            "i : [ 0.07055222 -0.02015138  0.36107434]\n",
            "learning : [ 0.33480474 -0.39423389 -0.43959196]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 3: extract embedding vectors from W1 and W2"
      ],
      "metadata": {
        "id": "hUNG_lEHWhtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute W3 as the average of W1 and W2 transposed\n",
        "W3 = (W1 + W2.T)/2\n",
        "\n",
        "# Print W3\n",
        "W3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLhxj1wOWLfv",
        "outputId": "730f3ec6-0fe0-4853-e69b-6b4dc4f4d890"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n",
              "       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n",
              "       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each word of the vocabulary\n",
        "for word in word2Ind:\n",
        "  # Extract the column corresponding to the index of the word in the vocabulary\n",
        "  word_embedding_vector = W3[:, word2Ind[word]]\n",
        "  # Print word alongside word embedding vector\n",
        "  print(f\"{word} : {word_embedding_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tteuKGscW91H",
        "outputId": "5ae3dea3-ca4f-4526-a89f-8c900414305a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am : [ 0.09752647 -0.05136565  0.19974284]\n",
            "because : [ 0.08665397  0.15459171 -0.03063173]\n",
            "happy : [-0.02389858 -0.15029611 -0.27839106]\n",
            "i : [0.1768788  0.19580601 0.12353994]\n",
            "learning : [ 0.3764029  -0.31673866 -0.04975536]\n"
          ]
        }
      ]
    }
  ]
}
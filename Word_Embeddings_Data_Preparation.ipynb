{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJFQJwzacCCpOS13tLOP4H"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUol6Dp3ycNZ",
        "outputId": "d2c91ea9-3a14-454f-9492-8f916411429c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/431.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m430.1/431.4 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc7H2chEwOiY",
        "outputId": "a068f15d-f1f8-4d7e-c933-dde9c6ff409f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "# Use to handle punctuation\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "LCB_dH1YwzrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning and tokenization"
      ],
      "metadata": {
        "id": "AoKFH4HRw8YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define corpus\n",
        "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!\"'"
      ],
      "metadata": {
        "id": "1f--63KNwxie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print original corpus\n",
        "print(f\"Corpus : {corpus}\")\n",
        "\n",
        "# Do the substitution\n",
        "data = re.sub(r'[,!?;-]', '.', corpus)\n",
        "\n",
        "# Print cleaned corpus\n",
        "print(f\"After cleaning punctuation : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctcw3Gx_xEVH",
        "outputId": "11628fba-3dc6-4003-8dd3-e8a7350fbffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus : Who ❤️ \"word embeddings\" in 2020? I do!!!\"\n",
            "After cleaning punctuation : Who ❤️ \"word embeddings\" in 2020. I do...\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the cleaned corpus\n",
        "print(f\"Initial string : {data}\")\n",
        "\n",
        "# Tokenize the cleaned corpus\n",
        "data = nltk.word_tokenize(data)\n",
        "\n",
        "# Print the tokenized version of the corpus\n",
        "print(f\"After tokenization : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxJ_FwxrxZHd",
        "outputId": "981a2149-32a5-4952-a0db-ba3f99f90729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial string : Who ❤️ \"word embeddings\" in 2020. I do...\"\n",
            "After tokenization : ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '...', \"''\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tokenized version of the corpus\n",
        "print(f\"Initial list of tokens : {data}\")\n",
        "\n",
        "# Filter tokenized corpus using list comprehension\n",
        "data  = [\n",
        "    ch.lower() for ch in data\n",
        "    if ch.isalpha()\n",
        "    or ch == '.'\n",
        "    or bool(emoji.emoji_list(ch))\n",
        "]\n",
        "\n",
        "# Print the tokenized and filtered version of the corpus\n",
        "print(f\"After cleaning : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy9-PFCsxyid",
        "outputId": "b662389d-98d3-4112-aaed-e3e9d1b213bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial list of tokens : ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '...', \"''\"]\n",
            "After cleaning : ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "  data = re.sub(r'[,!?;-]+', '.', corpus)\n",
        "  data = nltk.word_tokenize(data)\n",
        "  data = [\n",
        "      ch.lower() for ch in data\n",
        "      if ch.isalpha()\n",
        "      or ch == '.'\n",
        "      or bool(emoji.emoji_list(ch))\n",
        "  ]\n",
        "  return data"
      ],
      "metadata": {
        "id": "nl3EIhhFz6wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define new corpus\n",
        "corpus = \"I am happy because I am learning\"\n",
        "\n",
        "# Print new corpus\n",
        "print(f\"Corpus : {corpus}\")\n",
        "\n",
        "# Save tokenized version of corpus into 'words' variable\n",
        "words = tokenize(corpus)\n",
        "\n",
        "# Print the tokenzied version of the corpus\n",
        "print(f\"Words (tokens) : {words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVk4goGM0xwo",
        "outputId": "f12c24c5-2832-43c2-cd9c-3f5f8bb9201e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus : I am happy because I am learning\n",
            "Words (tokens) : ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this with any sentence\n",
        "tokenize(\"My name is John. How are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gadHGdoJ1TH-",
        "outputId": "d3a2b4e4-e331-4476-bee8-08de08898fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my', 'name', 'is', 'john', '.', 'how', 'are', 'you', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sliding window of words"
      ],
      "metadata": {
        "id": "Z6EQHpNl1aEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 'get_windows' function\n",
        "def get_windows(words, C):\n",
        "  i = C\n",
        "  while i < len(words) - C:\n",
        "    center_word = words[i]\n",
        "    context_words = words[(i - C) : i] + words[(i + 1) : (i + C + 1)]\n",
        "    yield context_words, center_word\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "FivvHlZy1bhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
        "  print(f\"{x}\\t{y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr5TiLIX2cgJ",
        "outputId": "391351df-d5fa-40ab-bb46-8cb02c5a0489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'because', 'i']\thappy\n",
            "['am', 'happy', 'i', 'am']\tbecause\n",
            "['happy', 'because', 'am', 'learning']\ti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\n",
        "for x, y in get_windows(tokenize(\"My name is John. How are you?\"), 1):\n",
        "    print(f'{x}\\t{y}')"
      ],
      "metadata": {
        "id": "RkZZgKSR2p6T",
        "outputId": "b9838294-7587-44a9-a47c-58c1e9bcbb4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my', 'is']\tname\n",
            "['name', 'john']\tis\n",
            "['is', '.']\tjohn\n",
            "['john', 'how']\t.\n",
            "['.', 'are']\thow\n",
            "['how', 'you']\tare\n",
            "['are', '.']\tyou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforming words into vectors for the training set"
      ],
      "metadata": {
        "id": "wYerhxY4CltZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(data):\n",
        "  words = sorted(list(set(data)))\n",
        "  n = len(words)\n",
        "  idx = 0\n",
        "\n",
        "  # return these correctly\n",
        "  word2Ind = {}\n",
        "  Ind2word = {}\n",
        "  for k in words:\n",
        "    word2Ind[k] = idx\n",
        "    Ind2word[idx] = k\n",
        "    idx +=1\n",
        "\n",
        "  return word2Ind, Ind2word"
      ],
      "metadata": {
        "id": "dwYk3wfaCJOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get \"word2Ind\" and \"Ind2word\" dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)"
      ],
      "metadata": {
        "id": "GjSMh5lUCjMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 'word2Ind' dictionary\n",
        "word2Ind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_GkOj0vC10d",
        "outputId": "c705187a-3cf5-4ef9-e3cc-066a4d853186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print value for the key 'i' within word2Ind dictionary\n",
        "print(\"Index of the word 'i' : \", word2Ind['i'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6JfYQcUDBdD",
        "outputId": "da9840f5-67c9-42d9-e3a9-298da8f81e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the word 'i' :  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 'Ind2word' dictionary\n",
        "Ind2word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfOHU1faDMhI",
        "outputId": "99c85273-481d-4227-bd2c-c0c26d5d6e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print value for the key '2' within Ind2word dictionary\n",
        "print(\"Word which has index 2 : \", Ind2word[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_MGHYthDR-L",
        "outputId": "040e5ea2-2315-4774-9353-fbfa3e2ec9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word which has index 2 :  happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save length of word2Ind dictionary into the 'V' variable\n",
        "V = len(word2Ind)\n",
        "\n",
        "# Print length of word2Ind dictionary\n",
        "print(\"Size of vacabulary : \", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YZ1pt_sDrxH",
        "outputId": "4c6e8722-34d7-4dcb-ae53-2ee331347bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vacabulary :  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting one-hot word vectors"
      ],
      "metadata": {
        "id": "IN-jhyLuD7pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save index of word 'happy' into the 'n' variable\n",
        "n = word2Ind['happy']\n",
        "\n",
        "# Print index of word 'happy'\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dMzLRphD60w",
        "outputId": "233c96f8-cc55-4dff-d77b-90cc0df4cf97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector with the same length as the vocabulary, filled with zeros\n",
        "center_word_vector = np.zeros(V)\n",
        "\n",
        "# Print vector\n",
        "center_word_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyKBcX90EE9W",
        "outputId": "3c08193f-24cb-4fe6-9dd8-bc9ee8190e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assert that the length of the vector is the same as the size of the vocabulary\n",
        "len(center_word_vector) == V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwCCsYU8EHrv",
        "outputId": "47817587-eda7-4d8d-eae3-a355f7f916a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace element number 'n' with a 1\n",
        "center_word_vector[n] = 1\n",
        "\n",
        "# Print vector\n",
        "center_word_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU7cVCkHEJ8O",
        "outputId": "cad73267-d9b9-4020-d492-b9f726915ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 'word_to_one_hot_vector' function\n",
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "  one_hot_vector = np.zeros(V)\n",
        "  one_hot_vector[word2Ind[word]] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "2bczearOENe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print output of 'word_to_one_hot_vector' function for word 'happy'\n",
        "word_to_one_hot_vector('happy', word2Ind, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoLb5IbLEiw7",
        "outputId": "0756165b-1f18-49c6-cf1e-704e52f668c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting context word vectors"
      ],
      "metadata": {
        "id": "YiVwl2U6EqmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define list containing context words\n",
        "context_words = ['i', 'am', 'because', 'i']"
      ],
      "metadata": {
        "id": "RC2__kyWEmMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one-hot vectors for each context word\n",
        "context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "\n",
        "# Print context words vectors\n",
        "context_words_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq7wAN-sFEbR",
        "outputId": "f262e9cc-4176-4f77-a55f-ba493798ec20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0., 0., 0., 1., 0.]),\n",
              " array([1., 0., 0., 0., 0.]),\n",
              " array([0., 1., 0., 0., 0.]),\n",
              " array([0., 0., 0., 1., 0.])]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean of vectors using numpy\n",
        "# Note the axis=0 parameter that tells mean to calculate the\n",
        "# average of the rows (if you had wanted the average of the columns,\n",
        "# you would have used axis=1).\n",
        "np.mean(context_words_vectors, axis = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQkIYWoBFcDA",
        "outputId": "01df16ea-b2f9-4186-ca54-d08af25a0aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 'context_words_to_vector' function\n",
        "def context_words_to_vector(context_words, word2Ind, V):\n",
        "  context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "  context_words_vectors = np.mean(context_words_vectors, axis = 0)\n",
        "  return context_words_vectors"
      ],
      "metadata": {
        "id": "HTjUim-6H5a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\n",
        "context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3aK5JxAJDSk",
        "outputId": "6b715106-b0b3-4cd2-9ca3-a715617bf528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the training set"
      ],
      "metadata": {
        "id": "GJiS4v1RJKzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print corpus\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31d3HsZrJEvA",
        "outputId": "9366fc71-2b78-4e3d-dc4d-400506ffdd66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for context_words, center_word in get_windows(words, 2):\n",
        "  print(f\"Context words : {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}\")\n",
        "  print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvS5wNqOJr0w",
        "outputId": "565b9a47-8680-4a3e-89fc-e03f38ce00ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context words : ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n",
            "Center word:  happy -> [0. 0. 1. 0. 0.]\n",
            "\n",
            "Context words : ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n",
            "Center word:  because -> [0. 1. 0. 0. 0.]\n",
            "\n",
            "Context words : ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n",
            "Center word:  i -> [0. 0. 0. 1. 0.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator function 'get_training_example'\n",
        "def get_training_example(words, C, word2Ind, V):\n",
        "  for context_words, center_word in get_windows(words, C):\n",
        "    yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
      ],
      "metadata": {
        "id": "6wVV0uE5KUnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print vectors associated to center and context words for corpus using the generator function\n",
        "for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n",
        "    print(f'Context words vector:  {context_words_vector}')\n",
        "    print(f'Center word vector:  {center_word_vector}')\n",
        "    print()"
      ],
      "metadata": {
        "id": "qpiV1GYLLES_",
        "outputId": "fa145dd8-0378-4c8b-87ad-1b7c552f61d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
            "Center word vector:  [0. 0. 1. 0. 0.]\n",
            "\n",
            "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
            "Center word vector:  [0. 1. 0. 0. 0.]\n",
            "\n",
            "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
            "Center word vector:  [0. 0. 0. 1. 0.]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
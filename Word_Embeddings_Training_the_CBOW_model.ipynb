{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYcDlPoOzrPZzNjCzMI4gD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wk9xN--V6wQ_"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(data):\n",
        "  words = sorted(list(set(data)))\n",
        "  n = len(words)\n",
        "  idx = 0\n",
        "  word2Ind = {}\n",
        "  Ind2word = {}\n",
        "  for k in words:\n",
        "    word2Ind[k] = idx\n",
        "    Ind2word[idx] = k\n",
        "    idx += 1\n",
        "  return word2Ind, Ind2word"
      ],
      "metadata": {
        "id": "EntnO0Vg9QzL"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the size of word embedding vectors\n",
        "N = 3\n",
        "\n",
        "# Define the size of vocabulary\n",
        "V = 5"
      ],
      "metadata": {
        "id": "IQRZrHcS66zw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization of weights and biases\n",
        "# Define the first matrix of weights\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "# Define second matrix of weigths\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "# Define first vector of biases\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "# Define second vector of biases\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ],
      "metadata": {
        "id": "ALx0PmCp7nWx"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'V (vocabulary size): {V}')\n",
        "print(f'N (embedding size / size of the hidden layer): {N}')\n",
        "print(f'size of W1: {W1.shape} (NxV)')\n",
        "print(f'size of b1: {b1.shape} (Nx1)')\n",
        "print(f'size of W2: {W2.shape} (VxN)')\n",
        "print(f'size of b2: {b2.shape} (Vx1)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zri1yks82Z8",
        "outputId": "ab6e0b4c-c6a5-40bf-c316-7a8a5d3af520"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V (vocabulary size): 5\n",
            "N (embedding size / size of the hidden layer): 3\n",
            "size of W1: (3, 5) (NxV)\n",
            "size of b1: (3, 1) (Nx1)\n",
            "size of W2: (5, 3) (VxN)\n",
            "size of b2: (5, 1) (Vx1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenized version of the corpus\n",
        "words = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n",
        "\n",
        "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "\n",
        "# Define the 'get_windows' function\n",
        "def get_windows(words, C):\n",
        "  i = C\n",
        "  while i < len(words) - C:\n",
        "    center_word = words[i]\n",
        "    context_words = words[(i - C) : i] + words[(i + 1) : (i + C + 1)]\n",
        "    yield context_words, center_word\n",
        "    i += 1\n",
        "\n",
        "# Define the 'word_to_one_hot_vector'\n",
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "  one_hot_vector = np.zeros(V)\n",
        "  one_hot_vector[word2Ind[word]] = 1\n",
        "  return one_hot_vector\n",
        "\n",
        "# Define the 'context_words_to_vector' function\n",
        "def context_words_to_vector(context_words, word2Ind, V):\n",
        "  context_words_vector = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "  context_words_vector = np.mean(context_words_vector, axis = 0)\n",
        "  return context_words_vector\n",
        "\n",
        "# Define the generator function 'get_training_examples'\n",
        "def get_training_example(words, C, word2Ind, V):\n",
        "  for context_words, center_word in get_windows(words, C):\n",
        "    yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
      ],
      "metadata": {
        "id": "D1BOevRg85mK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save generator object in the 'training_examples' variable with the desired arguments\n",
        "training_examples = get_training_example(words, 2, word2Ind, V)"
      ],
      "metadata": {
        "id": "icGwQxFEAEZb"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first value from genrator\n",
        "x_array, y_array = next(training_examples)"
      ],
      "metadata": {
        "id": "8jaBkfECAL9h"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the context words vector\n",
        "x_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itL2lfBVAQL6",
        "outputId": "72e0d0c4-2562-4ef6-d162-74d03b1fe5e0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print one hot vector of center word\n",
        "y_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tps8jwXkAaKu",
        "outputId": "e1220f22-582e-48b0-e864-917ad638fce0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy vector\n",
        "x = x_array.copy()\n",
        "\n",
        "# Reshape it\n",
        "x.shape = (V,1)\n",
        "\n",
        "# Print it\n",
        "print(f'x:\\n{x}\\n')\n",
        "\n",
        "# Copy vector\n",
        "y = y_array.copy()\n",
        "\n",
        "# Reshape it\n",
        "y.shape = (V,1)\n",
        "\n",
        "# Print it\n",
        "print(f'y:\\n{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCMR6XUPAe1G",
        "outputId": "dc211a1f-b6cd-45d2-aa1c-9f8924049d57"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:\n",
            "[[0.25]\n",
            " [0.25]\n",
            " [0.  ]\n",
            " [0.5 ]\n",
            " [0.  ]]\n",
            "\n",
            "y:\n",
            "[[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the \"relu\" function\n",
        "def relu(z):\n",
        "  result = z.copy()\n",
        "  result[result < 0] = 0\n",
        "  return result\n",
        "\n",
        "# Define softmax function\n",
        "def softmax(z):\n",
        "  e_z = np.exp(z)\n",
        "  sum_e_z = np.sum(e_z)\n",
        "  return e_z / sum_e_z"
      ],
      "metadata": {
        "id": "xoDg_sgADUxU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rq7lKxB0Dnue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}